{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f087e",
   "metadata": {},
   "source": [
    "# Open-Ended Response Analysis\n",
    "\n",
    "This notebook provides a basic analysis for the open-ended responses from the survey, namely:\n",
    "\n",
    "1. *‚ÄúWhat made the content believable or unbelievable to you?‚Äù*\n",
    "2. *‚ÄúWhat specific elements made the content credible or not?‚Äù*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234e4f1",
   "metadata": {},
   "source": [
    "## üîπ Step 4: Frequency Table or Word Cloud (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832c790",
   "metadata": {},
   "source": [
    "### Load preprocessed data\n",
    "\n",
    "To start, we'll import all relevant libraries needed to test this hypothesis and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6562d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing import load_and_preprocess\n",
    "from random import sample\n",
    "\n",
    "# Load data\n",
    "df = load_and_preprocess('data/cleaned_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0677b",
   "metadata": {},
   "source": [
    "### Combine and inspect answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690e435f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59    I tend to believe content that supports my alr...\n",
       "3     I was feeling strongly towards statements that...\n",
       "53    if the content contained numbers or reasons su...\n",
       "46    all of the content ist not really beliveable, ...\n",
       "47    Unbelievable- the missing of context and sourc...\n",
       "Name: open_response, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine open-ended responses into one column\n",
    "df['open_response'] = df['FS05_01'].fillna('') + ' ' + df['FS06_01'].fillna('')\n",
    "\n",
    "# Optional: inspect a few rows\n",
    "df['open_response'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1497e",
   "metadata": {},
   "source": [
    "### Frequency Count\n",
    "\n",
    "The following cell creates a frequency count using basic keyword tagging to get an insight on what aspects of textual content caused (un)believability or (un)credibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d238aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response_tags\n",
       "factual references     11\n",
       "emotional tone          6\n",
       "language/style          6\n",
       "argument quality        5\n",
       "subjective/personal     4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple keyword tagging example\n",
    "def tag_reason(text):\n",
    "    text = text.lower()\n",
    "    tags = []\n",
    "    if 'language' in text or 'style' in text:\n",
    "        tags.append('language/style')\n",
    "    if 'facts' in text or 'data' in text:\n",
    "        tags.append('factual references')\n",
    "    if 'emotion' in text or 'manipulativ' in text:\n",
    "        tags.append('emotional tone')\n",
    "    if 'structure' in text or 'logic' in text:\n",
    "        tags.append('argument quality')\n",
    "    if 'personal' in text:\n",
    "        tags.append('subjective/personal')\n",
    "    return tags\n",
    "\n",
    "# Apply tagging\n",
    "df['response_tags'] = df['open_response'].apply(tag_reason)\n",
    "df['response_tags'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1ba18",
   "metadata": {},
   "source": [
    "To visualize and understand those aspects, we'll sample two examples for each tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b13f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Kategorie: LANGUAGE/STYLE\n",
      "‚Ä¢ If it aligned with my knowledge and was presented in an unbiased sounding language Facts, numbers and then normally sources, but there were no sources stated at all. Therefore all content seemed only slightly credible.\n",
      "‚Ä¢ The way it was written. Especially the writing style being rather objective, leaving it clear that a specific opinion is presented in a non-judgmental way or way that feels like invading your own personal space of opinion.  Presenting opinions as facts (as unfortunately often done by many humans) makes content not credible to me. Also a clear line of argumentation which makes it easy to follow and then judge whether to agree with the view offered or not.\n",
      "\n",
      "üîπ Kategorie: FACTUAL REFERENCES\n",
      "‚Ä¢ Whether reasoning is used, that I know is true and whether the content supports my existing opinion. Naming sources and more specific data would have made it more credible.\n",
      "‚Ä¢ mostly if there were points that followed from each other to support each other made it more beliveable. factual evidence or facts that held more overall truth\n",
      "\n",
      "üîπ Kategorie: ARGUMENT QUALITY\n",
      "‚Ä¢ complexity of the vocabulary and sentence structure, as well as listings Whether the facts seemed logical and the emotionality\n",
      "‚Ä¢ my prior knowledge and opinion about the topic the structure of the quote and my prior knowledge\n",
      "\n",
      "üîπ Kategorie: EMOTIONAL TONE\n",
      "‚Ä¢ I think what I most look for is emotion to make content believable. If a person feels strongly about a topic, you can often see it in their language. Emotion, Formality\n",
      "‚Ä¢ Writing Emotional topics\n",
      "\n",
      "üîπ Kategorie: SUBJECTIVE/PERSONAL\n",
      "‚Ä¢ It often seemed to be a personal opinion which was stated, in combination with facts. For assuming if the author was human, I focussed on the formulation. The form of the argument.\n",
      "‚Ä¢ The way it was written. Especially the writing style being rather objective, leaving it clear that a specific opinion is presented in a non-judgmental way or way that feels like invading your own personal space of opinion.  Presenting opinions as facts (as unfortunately often done by many humans) makes content not credible to me. Also a clear line of argumentation which makes it easy to follow and then judge whether to agree with the view offered or not.\n"
     ]
    }
   ],
   "source": [
    "# Get list of all unique tags\n",
    "unique_tags = set(tag for sublist in df['response_tags'] for tag in sublist)\n",
    "\n",
    "# print two examples per tag\n",
    "for tag in unique_tags:\n",
    "    print(f\"\\nüîπ Kategorie: {tag.upper()}\")\n",
    "   \n",
    "    matching = df[df['response_tags'].apply(lambda x: tag in x)]\n",
    "    \n",
    "    examples = sample(list(matching['open_response']), k=min(2, len(matching)))\n",
    "    for quote in examples:\n",
    "        print(f\"‚Ä¢ {quote}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac64f01",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Factual references, tone, style, and argument structure were the most commonly cited factors. Some participants emphasized emotional language or personal resonance as cues for credibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
